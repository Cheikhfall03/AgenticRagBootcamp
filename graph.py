import time
import traceback
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

# --- Import des composants du graphe ---
from chains.answer_grader import answer_grader
from chains.retriever_grader import retrieval_grader
from chains.router_query import question_router
from chains.hallucination_grader import hallucination_grader
from nodes.generate import generate
from nodes.web_search import web_search
from nodes.query_rewrite import query_rewrite
from Node_constant import RETRIEVE, GRADE_DOCUMENTS, GENERATE, WEBSEARCH, QUERY_REWRITE
from state import GraphState

# --- Import de l'initialiseur du retriever par d√©faut ---
from ingestion.ingestion import initialize_default_retriever

load_dotenv()

class AdaptiveRAGSystem:
    """Syst√®me RAG modulaire, robuste et centralis√©."""
    def __init__(self):
        """Initialise le syst√®me RAG avec un retriever par d√©faut."""
        self.workflow = StateGraph(GraphState)
        # --- Initialisation du retriever par d√©faut ---
        try:
            self.default_retriever = initialize_default_retriever()
        except Exception as e:
            print(f"‚ùå ERREUR CRITIQUE: Impossible d'initialiser le retriever par d√©faut: {e}")
            self.default_retriever = None

        self.current_retriever = self.default_retriever

        # --- Construction du graphe ---
        self._setup_workflow()

        # --- Compilation du graphe ---
        memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=memory)
        print("‚úÖ Graphe LangGraph compil√© avec succ√®s.")

    def _retrieve_documents(self, state: GraphState) -> Dict[str, Any]:
        """
        R√©cup√®re les documents en utilisant le retriever actuellement actif (soit par d√©faut, soit personnalis√©).
        Ce n≈ìud est maintenant interne √† la classe et fiable.
        """
        print("---N≈íUD: R√âCUP√âRATION DE DOCUMENTS---")
        question = state["question"]

        if self.current_retriever is None:
            print("‚ö†Ô∏è Aucun retriever n'est disponible. Retourne une liste vide.")
            return {"documents": []}

        try:
            print(f"üîé Utilisation du retriever: {type(self.current_retriever)}")
            documents = self.current_retriever.invoke(question)
            print(f"‚úÖ {len(documents)} document(s) r√©cup√©r√©(s).")
            return {"documents": documents}
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration de documents: {e}")
            return {"documents": []}

    def _grade_documents(self, state: GraphState) -> Dict[str, Any]:
        print("---N≈íUD: √âVALUATION DE LA PERTINENCE DES DOCUMENTS---")
        question = state["question"]
        documents = state["documents"]

        if not documents:
            return {"documents": []}

        filtered_docs = []
        for d in documents:
            try:
                doc_text = getattr(d, "page_content", str(d))
                score = retrieval_grader.invoke({"question": question, "document": doc_text})
                if getattr(score, "binary_score", False):
                    print("‚úÖ Document pertinent.")
                    filtered_docs.append(d)
                else:
                    print("‚õî Document non pertinent.")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur d'√©valuation, inclusion par d√©faut: {e}")
                filtered_docs.append(d) # En cas d'erreur, on garde le doc par s√©curit√©

        return {"documents": filtered_docs}

    def _route_question(self, state: GraphState) -> str:
        """
        Route toujours vers la r√©cup√©ration de documents, car nous avons toujours un retriever.
        """
        print("---N≈íUD: ROUTAGE DE LA QUESTION---")
        print("‚û°Ô∏è Toujours router vers RETRIEVE car un retriever (d√©faut ou custom) est toujours disponible.")
        return RETRIEVE

    def _decide_to_generate(self, state: GraphState) -> str:
        """
        D√©cide s'il faut g√©n√©rer une r√©ponse, r√©√©crire la question, ou passer √† la recherche web.
        """
        print("---N≈íUD: D√âCISION POST-R√âCUP√âRATION---")
        if state["documents"]:
            print("‚úÖ Documents pertinents trouv√©s. Passage √† la g√©n√©ration.")
            return GENERATE
        else:
            if state["query_rewrite_count"] < 1: # On autorise 1 r√©√©criture
                print("‚õî Aucun document pertinent. Tentative de r√©√©criture de la question.")
                return QUERY_REWRITE
            else:
                print("‚õî √âchec apr√®s r√©√©criture. Passage √† la recherche web comme dernier recours.")
                return WEBSEARCH

    def _grade_generation(self, state: GraphState) -> str:
        """Evaluates the generation for hallucinations and relevance."""
        print("---NODE: GRADE GENERATION---")
        question = state["question"]
        documents = state["documents"]
        generation = state["generation"]

        if not generation:
            print("‚õî Generation is empty. Ending.")
            return END

        # If the generation came from a web search, we can't do a hallucination check.
        # We just check if the answer is useful.
        if not documents:
            print("‚ö†Ô∏è No documents to check for hallucinations (likely from web search).")
            answer_score = answer_grader.invoke({"question": question, "generation": generation})
            if getattr(answer_score, "binary_score", False):
                print("‚úÖ Web search answer deemed USEFUL.")
                return END
            else:
                print("‚õî Web search answer deemed NOT USEFUL.")
                return END

        # Combine and truncate the document context to prevent exceeding the model's token limit.
        MAX_CONTEXT_CHARS = 15000  # Approx. 3750 tokens, a safe limit for most models.
        docs_texts = [getattr(d, "page_content", str(d)) for d in documents]
        full_context = "\n\n---\n\n".join(docs_texts)

        if len(full_context) > MAX_CONTEXT_CHARS:
            print(f"‚ö†Ô∏è Context length ({len(full_context)} chars) is too long. Truncating to {MAX_CONTEXT_CHARS}.")
            full_context = full_context[:MAX_CONTEXT_CHARS]

        print("---CHECKING FOR HALLUCINATIONS---")
        if isinstance(full_context, list):
            documents_text = "\n\n".join([doc.page_content for doc in full_context])
        else:
            documents_text = str(full_context)

        try:
            hallucination_score = hallucination_grader.invoke({
                "documents": documents_text[:5000],  # üîí S√©curise la taille du contexte
                "generation": generation[:2000]      # üîí Limite aussi la g√©n√©ration
            })
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur dans hallucination_grader: {e}")
            # On consid√®re par d√©faut que c‚Äôest grounded pour ne pas bloquer
            return END

        if not getattr(hallucination_score, "binary_score", False):
            print("‚õî DECISION: HALLUCINATION DETECTED. Re-trying generation.")
            if state["generation_count"] < 1: # Allow one retry
                return GENERATE
            else:
                print("‚õî Failed to correct hallucination after retry. Ending.")
                return END

        print("‚úÖ DECISION: GENERATION IS GROUNDED in documents.")
        print("---CHECKING FOR ANSWER RELEVANCE---")
        answer_score = answer_grader.invoke({"question": question, "generation": generation})
        if getattr(answer_score, "binary_score", False):
            print("‚úÖ Answer is RELEVANT and GROUNDED. Ending.")
            return END
        else:
            print("‚õî Answer is NOT RELEVANT, but grounded. Ending.")
            return END

    def _setup_workflow(self):
        """Construit et connecte les n≈ìuds du graphe LangGraph."""
        self.workflow.set_entry_point(RETRIEVE) # <--- POINT D'ENTR√âE SIMPLIFI√â

        self.workflow.add_node(RETRIEVE, self._retrieve_documents)
        self.workflow.add_node(GRADE_DOCUMENTS, self._grade_documents)
        self.workflow.add_node(QUERY_REWRITE, query_rewrite)
        self.workflow.add_node(WEBSEARCH, web_search)
        self.workflow.add_node(GENERATE, generate)

        # --- D√©finition des chemins (edges) ---
        self.workflow.add_edge(RETRIEVE, GRADE_DOCUMENTS)
        self.workflow.add_edge(QUERY_REWRITE, RETRIEVE)
        self.workflow.add_edge(WEBSEARCH, GENERATE)

        self.workflow.add_conditional_edges(
            GRADE_DOCUMENTS,
            self._decide_to_generate,
            {
                GENERATE: GENERATE,
                QUERY_REWRITE: QUERY_REWRITE,
                WEBSEARCH: WEBSEARCH
            }
        )
        self.workflow.add_conditional_edges(
            GENERATE,
            self._grade_generation,
            {
                GENERATE: GENERATE,
                END: END
            }
        )

    def ask_question(self, question: str, retriever: Optional[Any] = None, config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Point d'entr√©e principal pour poser une question au syst√®me.
        G√®re intelligemment le retriever √† utiliser.
        """
        if not self.app:
            return {"success": False, "answer": "Erreur: Le syst√®me RAG n'est pas compil√©."}

        # Si un retriever est fourni (fichiers upload√©s), on l'utilise.
        # Sinon, on utilise le retriever par d√©faut initialis√© au d√©marrage.
        self.current_retriever = retriever if retriever is not None else self.default_retriever

        if self.current_retriever is None:
            return {"success": False, "answer": "Erreur: Aucun retriever n'est disponible (ni par d√©faut, ni personnalis√©)."}

        initial_state = {"question": question, "query_rewrite_count": 0, "generation_count": 0}

        print(f"--- Lancement du graphe pour la question: '{question}' ---")
        start_time = time.time()

        final_state = None
        try:
            for event in self.app.stream(initial_state, config=config, stream_mode="values"):
                final_state = event

            end_time = time.time()

            answer = final_state.get("generation", "Aucune r√©ponse n'a pu √™tre g√©n√©r√©e.")
            docs_used = final_state.get("documents", [])

            return {
                "success": True,
                "answer": answer,
                "processing_time": round(end_time - start_time, 2),
                "documents_used": len(docs_used),
                "query_rewrites": final_state.get("query_rewrite_count", 0),
                "documents": docs_used,
            }
        except Exception as e:
            end_time = time.time()
            print(f"--- ERREUR D'EX√âCUTION DU GRAPHE: {e} ---")
            traceback.print_exc()
            return {
                "success": False,
                "answer": f"Une erreur est survenue: {e}",
                "processing_time": round(end_time - start_time, 2),
            }

# --- Instance unique (Singleton) pour l'application ---
rag_system = AdaptiveRAGSystem()
