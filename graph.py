import time

import traceback

from typing import Dict, Any, Optional

from dotenv import load_dotenv

from langgraph.graph import END, StateGraph

from langgraph.checkpoint.memory import MemorySaver



# --- Import des composants du graphe ---

from chains.answer_grader import answer_grader

from chains.retriever_grader import retrieval_grader

from chains.router_query import question_router

from chains.hallucination_grader import hallucination_grader

from nodes.generate import generate

from nodes.web_search import web_search

from nodes.query_rewrite import query_rewrite

from Node_constant import RETRIEVE, GRADE_DOCUMENTS, GENERATE, WEBSEARCH, QUERY_REWRITE

from state import GraphState



# --- Import de l'initialiseur du retriever par dÃ©faut ---

from ingestion.ingestion import initialize_default_retriever



load_dotenv()



class AdaptiveRAGSystem:

Â  Â  """SystÃ¨me RAG modulaire, robuste et centralisÃ©."""

Â  Â Â 

Â  Â  def __init__(self):

Â  Â  Â  Â  """Initialise le systÃ¨me RAG avec un retriever par dÃ©faut."""

Â  Â  Â  Â  self.workflow = StateGraph(GraphState)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # --- Initialisation du retriever par dÃ©faut ---

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  self.default_retriever = initialize_default_retriever()

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  print(f"âŒ ERREUR CRITIQUE: Impossible d'initialiser le retriever par dÃ©faut: {e}")

Â  Â  Â  Â  Â  Â  self.default_retriever = None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  self.current_retriever = self.default_retriever

Â  Â  Â  Â Â 

Â  Â  Â  Â  # --- Construction du graphe ---

Â  Â  Â  Â  self._setup_workflow()

Â  Â  Â  Â Â 

Â  Â  Â  Â  # --- Compilation du graphe ---

Â  Â  Â  Â  memory = MemorySaver()

Â  Â  Â  Â  self.app = self.workflow.compile(checkpointer=memory)

Â  Â  Â  Â  print("âœ… Graphe LangGraph compilÃ© avec succÃ¨s.")



Â  Â  def _retrieve_documents(self, state: GraphState) -> Dict[str, Any]:

Â  Â  Â  Â  """

Â  Â  Â  Â  RÃ©cupÃ¨re les documents en utilisant le retriever actuellement actif (soit par dÃ©faut, soit personnalisÃ©).

Â  Â  Â  Â  Ce nÅ“ud est maintenant interne Ã  la classe et fiable.

Â  Â  Â  Â  """

Â  Â  Â  Â  print("---NÅ’UD: RÃ‰CUPÃ‰RATION DE DOCUMENTS---")

Â  Â  Â  Â  question = state["question"]

Â  Â  Â  Â Â 

Â  Â  Â  Â  if self.current_retriever is None:

Â  Â  Â  Â  Â  Â  print("âš ï¸ Aucun retriever n'est disponible. Retourne une liste vide.")

Â  Â  Â  Â  Â  Â  return {"documents": []}

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  print(f"ğŸ” Utilisation du retriever: {type(self.current_retriever)}")

Â  Â  Â  Â  Â  Â  documents = self.current_retriever.invoke(question)

Â  Â  Â  Â  Â  Â  print(f"âœ… {len(documents)} document(s) rÃ©cupÃ©rÃ©(s).")

Â  Â  Â  Â  Â  Â  return {"documents": documents}

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  print(f"âŒ Erreur lors de la rÃ©cupÃ©ration de documents: {e}")

Â  Â  Â  Â  Â  Â  return {"documents": []}



Â  Â  # ... (les autres fonctions du graphe comme _grade_documents, etc. restent les mÃªmes) ...



Â  Â  def _grade_documents(self, state: GraphState) -> Dict[str, Any]:

Â  Â  Â  Â  print("---NÅ’UD: Ã‰VALUATION DE LA PERTINENCE DES DOCUMENTS---")

Â  Â  Â  Â  question = state["question"]

Â  Â  Â  Â  documents = state["documents"]

Â  Â  Â  Â Â 

Â  Â  Â  Â  if not documents:

Â  Â  Â  Â  Â  Â  return {"documents": []}

Â  Â  Â  Â Â 

Â  Â  Â  Â  filtered_docs = []

Â  Â  Â  Â  for d in documents:

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  doc_text = getattr(d, "page_content", str(d))

Â  Â  Â  Â  Â  Â  Â  Â  score = retrieval_grader.invoke({"question": question, "document": doc_text})

Â  Â  Â  Â  Â  Â  Â  Â  if getattr(score, "binary_score", False):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("âœ… Document pertinent.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filtered_docs.append(d)

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("â›” Document non pertinent.")

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Erreur d'Ã©valuation, inclusion par dÃ©faut: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  filtered_docs.append(d) # En cas d'erreur, on garde le doc par sÃ©curitÃ©

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  return {"documents": filtered_docs}

Â  Â Â 

Â  Â  def _route_question(self, state: GraphState) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  Route toujours vers la rÃ©cupÃ©ration de documents, car nous avons toujours un retriever.

Â  Â  Â  Â  """

Â  Â  Â  Â  print("---NÅ’UD: ROUTAGE DE LA QUESTION---")

Â  Â  Â  Â  print("â¡ï¸ Toujours router vers RETRIEVE car un retriever (dÃ©faut ou custom) est toujours disponible.")

Â  Â  Â  Â  return RETRIEVE



Â  Â  def _decide_to_generate(self, state: GraphState) -> str:

Â  Â  Â  Â  """

Â  Â  Â  Â  DÃ©cide s'il faut gÃ©nÃ©rer une rÃ©ponse, rÃ©Ã©crire la question, ou passer Ã  la recherche web.

Â  Â  Â  Â  """

Â  Â  Â  Â  print("---NÅ’UD: DÃ‰CISION POST-RÃ‰CUPÃ‰RATION---")

Â  Â  Â  Â  if state["documents"]:

Â  Â  Â  Â  Â  Â  print("âœ… Documents pertinents trouvÃ©s. Passage Ã  la gÃ©nÃ©ration.")

Â  Â  Â  Â  Â  Â  return GENERATE

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  if state["query_rewrite_count"] < 1: # On autorise 1 rÃ©Ã©criture

Â  Â  Â  Â  Â  Â  Â  Â  Â print("â›” Aucun document pertinent. Tentative de rÃ©Ã©criture de la question.")

Â  Â  Â  Â  Â  Â  Â  Â  Â return QUERY_REWRITE

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â print("â›” Ã‰chec aprÃ¨s rÃ©Ã©criture. Passage Ã  la recherche web comme dernier recours.")

Â  Â  Â  Â  Â  Â  Â  Â  Â return WEBSEARCH

Â  Â Â 

Â  Â  # In graph.py



Â  Â  def _grade_generation(self, state: GraphState) -> str:

Â  Â  Â  Â  """Evaluates the generation for hallucinations and relevance."""

Â  Â  Â  Â  print("---NODE: GRADE GENERATION---")

Â  Â  Â  Â  question = state["question"]

Â  Â  Â  Â  documents = state["documents"]

Â  Â  Â  Â  generation = state["generation"]

Â  Â Â 

Â  Â  Â  Â  if not generation:

Â  Â  Â  Â  Â  Â  print("â›” Generation is empty. Ending.")

Â  Â  Â  Â  Â  Â  return END

Â  Â Â 

Â  Â  Â  Â  # If the generation came from a web search, we can't do a hallucination check.

Â  Â  Â  Â  # We just check if the answer is useful.

Â  Â  Â  Â  if not documents:

Â  Â  Â  Â  Â  Â  print("âš ï¸ No documents to check for hallucinations (likely from web search).")

Â  Â  Â  Â  Â  Â  answer_score = answer_grader.invoke({"question": question, "generation": generation})

Â  Â  Â  Â  Â  Â  if getattr(answer_score, "binary_score", False):

Â  Â  Â  Â  Â  Â  Â  Â  print("âœ… Web search answer deemed USEFUL.")

Â  Â  Â  Â  Â  Â  Â  Â  return END

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  print("â›” Web search answer deemed NOT USEFUL.")

Â  Â  Â  Â  Â  Â  Â  Â  return END

Â  Â Â 

Â  Â  Â  Â  # --- FIX APPLIED HERE ---

Â  Â  Â  Â  # Combine and truncate the document context to prevent exceeding the model's token limit.

Â  Â  Â  Â  # This is the primary cause of the groq.BadRequestError.

Â  Â  Â  Â  MAX_CONTEXT_CHARS = 15000Â  # Approx. 3750 tokens, a safe limit for most models.

Â  Â  Â  Â  docs_texts = [getattr(d, "page_content", str(d)) for d in documents]

Â  Â  Â  Â  full_context = "\n\n---\n\n".join(docs_texts)

Â  Â Â 

Â  Â  Â  Â  if len(full_context) > MAX_CONTEXT_CHARS:

Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Context length ({len(full_context)} chars) is too long. Truncating to {MAX_CONTEXT_CHARS}.")

Â  Â  Â  Â  Â  Â  full_context = full_context[:MAX_CONTEXT_CHARS]

Â  Â  Â  Â Â 

Â  Â  Â  Â  print("---CHECKING FOR HALLUCINATIONS---")

Â  Â  Â  Â  # We now pass the potentially truncated context as a single item in a list.

Â  Â  Â  Â  # Dans graph.py, ligne 158 environ

Â  Â  Â  Â  if isinstance(full_context, list):Â Â 

Â  Â  Â  Â  Â  Â  documents_text = "\n\n".join([doc.page_content for doc in full_context])Â Â 

Â  Â  Â  Â  else:Â Â 

Â  Â  Â  Â  Â  Â  documents_text = str(full_context)

Â  Â  Â  Â Â 

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  hallucination_score = hallucination_grader.invoke({

Â  Â  Â  Â  Â  Â  Â  Â  "documents": documents_text[:5000],Â  # ğŸ”’ SÃ©curise la taille du contexte

Â  Â  Â  Â  Â  Â  Â  Â  "generation": generation[:2000]Â  Â  Â  # ğŸ”’ Limite aussi la gÃ©nÃ©ration

Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Erreur dans hallucination_grader: {e}")

Â  Â  Â  Â  Â  Â  # On considÃ¨re par dÃ©faut que câ€™est grounded pour ne pas bloquer

Â  Â  Â  Â  Â  Â  return END





Â  Â Â 

Â  Â  Â  Â  # The rest of the logic proceeds as before.

Â  Â  Â  Â  if not getattr(hallucination_score, "binary_score", False):

Â  Â  Â  Â  Â  Â  print("â›” DECISION: HALLUCINATION DETECTED. Re-trying generation.")

Â  Â  Â  Â  Â  Â  if state["generation_count"] < 1: # Allow one retry

Â  Â  Â  Â  Â  Â  Â  Â  return GENERATE

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  print("â›” Failed to correct hallucination after retry. Ending.")

Â  Â  Â  Â  Â  Â  Â  Â  return END

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  print("âœ… DECISION: GENERATION IS GROUNDED in documents.")

Â  Â  Â  Â  print("---CHECKING FOR ANSWER RELEVANCE---")

Â  Â  Â  Â  answer_score = answer_grader.invoke({"question": question, "generation": generation})

Â  Â  Â  Â  if getattr(answer_score, "binary_score", False):

Â  Â  Â  Â  Â  Â  print("âœ… Answer is RELEVANT and GROUNDED. Ending.")

Â  Â  Â  Â  Â  Â  return END

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  print("â›” Answer is NOT RELEVANT, but grounded. Ending.")

Â  Â  Â  Â  Â  Â  return END



Â  Â  def _setup_workflow(self):

Â  Â  Â  Â  """Construit et connecte les nÅ“uds du graphe LangGraph."""

Â  Â  Â  Â  self.workflow.set_entry_point(RETRIEVE) # <--- POINT D'ENTRÃ‰E SIMPLIFIÃ‰

Â  Â  Â  Â Â 

Â  Â  Â  Â  self.workflow.add_node(RETRIEVE, self._retrieve_documents)

Â  Â  Â  Â  self.workflow.add_node(GRADE_DOCUMENTS, self._grade_documents)

Â  Â  Â  Â  self.workflow.add_node(QUERY_REWRITE, query_rewrite)

Â  Â  Â  Â  self.workflow.add_node(WEBSEARCH, web_search)

Â  Â  Â  Â  self.workflow.add_node(GENERATE, generate)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # --- DÃ©finition des chemins (edges) ---

Â  Â  Â  Â  self.workflow.add_edge(RETRIEVE, GRADE_DOCUMENTS)

Â  Â  Â  Â  self.workflow.add_edge(QUERY_REWRITE, RETRIEVE)

Â  Â  Â  Â  self.workflow.add_edge(WEBSEARCH, GENERATE)

Â  Â  Â  Â Â 

Â  Â  Â  Â  self.workflow.add_conditional_edges(

Â  Â  Â  Â  Â  Â  GRADE_DOCUMENTS,

Â  Â  Â  Â  Â  Â  self._decide_to_generate,

Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  GENERATE: GENERATE,

Â  Â  Â  Â  Â  Â  Â  Â  QUERY_REWRITE: QUERY_REWRITE,

Â  Â  Â  Â  Â  Â  Â  Â  WEBSEARCH: WEBSEARCH

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  )

Â  Â  Â  Â  self.workflow.add_conditional_edges(

Â  Â  Â  Â  Â  Â  GENERATE,

Â  Â  Â  Â  Â  Â  self._grade_generation,

Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  GENERATE: GENERATE,

Â  Â  Â  Â  Â  Â  Â  Â  END: END

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  def ask_question(self, question: str, retriever: Optional[Any] = None, config: Optional[Dict] = None) -> Dict[str, Any]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Point d'entrÃ©e principal pour poser une question au systÃ¨me.

Â  Â  Â  Â  GÃ¨re intelligemment le retriever Ã  utiliser.

Â  Â  Â  Â  """

Â  Â  Â  Â  if not self.app:

Â  Â  Â  Â  Â  Â  return {"success": False, "answer": "Erreur: Le systÃ¨me RAG n'est pas compilÃ©."}

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  # --- LOGIQUE CENTRALE CORRIGÃ‰E ---

Â  Â  Â  Â  # Si un retriever est fourni (fichiers uploadÃ©s), on l'utilise.

Â  Â  Â  Â  # Sinon, on utilise le retriever par dÃ©faut initialisÃ© au dÃ©marrage.

Â  Â  Â  Â  self.current_retriever = retriever if retriever is not None else self.default_retriever

Â  Â  Â  Â Â 

Â  Â  Â  Â  if self.current_retriever is None:

Â  Â  Â  Â  Â  Â  Â return {"success": False, "answer": "Erreur: Aucun retriever n'est disponible (ni par dÃ©faut, ni personnalisÃ©)."}



Â  Â  Â  Â  initial_state = {"question": question, "query_rewrite_count": 0, "generation_count": 0}

Â  Â  Â  Â Â 

Â  Â  Â  Â  print(f"--- Lancement du graphe pour la question: '{question}' ---")

Â  Â  Â  Â  start_time = time.time()

Â  Â  Â  Â Â 

Â  Â  Â  Â  final_state = None

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  for event in self.app.stream(initial_state, config=config, stream_mode="values"):

Â  Â  Â  Â  Â  Â  Â  Â  final_state = event

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  end_time = time.time()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  answer = final_state.get("generation", "Aucune rÃ©ponse n'a pu Ãªtre gÃ©nÃ©rÃ©e.")

Â  Â  Â  Â  Â  Â  docs_used = final_state.get("documents", [])

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  Â  Â  Â  Â  "answer": answer,

Â  Â  Â  Â  Â  Â  Â  Â  "processing_time": round(end_time - start_time, 2),

Â  Â  Â  Â  Â  Â  Â  Â  "documents_used": len(docs_used),

Â  Â  Â  Â  Â  Â  Â  Â  "query_rewrites": final_state.get("query_rewrite_count", 0),

Â  Â  Â  Â  Â  Â  Â  Â  "documents": docs_used,

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  end_time = time.time()

Â  Â  Â  Â  Â  Â  print(f"--- ERREUR D'EXÃ‰CUTION DU GRAPHE: {e} ---")

Â  Â  Â  Â  Â  Â  traceback.print_exc()

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "success": False,

Â  Â  Â  Â  Â  Â  Â  Â  "answer": f"Une erreur est survenue: {e}",

Â  Â  Â  Â  Â  Â  Â  Â  "processing_time": round(end_time - start_time, 2),

Â  Â  Â  Â  Â  Â  }



# --- Instance unique (Singleton) pour l'application ---

rag_system = AdaptiveRAGSystem() 
